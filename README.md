# Project Overview

## bert.ipynb

This is a Jupyter Notebook file that contains code as example for working with the BERT (Bidirectional Encoder Representations from Transformers) model. BERT is a transformer-based model designed for NLP tasks such as text classification, question answering, and language modeling. The notebook might include steps for loading the BERT model, preprocessing text data, training the model on a dataset, and evaluating its performance. In this file is possible to see how BERT is util for models.

## gpt2

This file contains code related to the GPT-2 (Generative Pre-trained Transformer 2) model. GPT-2 is another transformer-based model, but it is primarily used for generating human-like text. The content might include scripts for loading the GPT-2 model, generating text based on a given prompt, fine-tuning the model on a specific dataset, and evaluating the generated text.

## text_summary

This file contains code for summarizing text. It might implement or utilize various text summarization techniques, such as extractive summarization (selecting key sentences from the original text) or abstractive summarization (generating a concise summary that captures the main ideas). The content could include functions or classes for preprocessing text, applying summarization algorithms, and evaluating the quality of the summaries.